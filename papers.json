[
  {
    "title": "Animal Avatars: Reconstructing Animatable 3D Animals from Casual Videos",
    "abstract": "We present a method to build animatable dog avatars from monocular videos.\n\nThis is challenging as animals display a range of (unpredictable) non-rigid movements and have a variety of appearance details (e.g., fur, spots, tails). We develop an approach that links the video frames via a 4D solution that jointly solves for animal's pose variation, and its appearance (in a canonical pose).\n\nTo this end, we significantly improve the quality of template-based shape fitting by endowing the SMAL parametric model with Continuous Surface Embeddings (CSE), which brings image-to-mesh reprojection constaints that are denser, and thus stronger, than the previously used sparse semantic keypoint correspondences. To model appearance, we propose an implicit duplex-mesh texture that is defined in the canonical pose, but can be deformed using SMAL pose coefficients and later rendered to enforce a photometric compatibility with the input video frames . On the challenging CoP3D dataset, we demonstrate superior results (both in terms of pose estimates and predicted appearance) to existing template-free (RAC) and template-based approaches (BARC, BITE).",
    "notes": "- Takes 2 hours per scene\n- Template Based\n- mono video reconstruction\n\n- inverse rendering\n- framework that jointly recovers an animal’s intrinsic shape, time-varying pose, and detailed texture from casual monocular videos\n- enhances the SMAL model with Continuous Surface Embeddings\n- implicit duplex-mesh texture model\n- decouples the animal’s static shape from its dynamic motion by factoring out camera movement with Structure-from-Motion data, resulting in smoother temporal reconstruction\n\n",
    "link": "https://arxiv.org/pdf/2403.17103",
    "codeLink": "https://remysabathier.github.io/animalavatar.github.io/",
    "bibtex": "@inproceedings{sabathier2024animal,\n  title={Animal avatars: Reconstructing animatable 3D animals from casual videos},\n  author={Sabathier, Remy and Mitra, Niloy J and Novotny, David},\n  booktitle={European Conference on Computer Vision},\n  pages={270--287},\n  year={2024},\n  organization={Springer}\n}"
  },
  {
    "title": "Animatable Implicit Neural Representations for Creating Realistic Avatars from Videos",
    "abstract": "This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a non-rigidly deforming scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce a pose-driven deformation field based on the linear blend skinning algorithm, which combines the blend weight field and the 3D human skeleton to produce observation-to-canonical correspondences. Since 3D human skeletons are more observable, they can regularize the learning of the deformation field. Moreover, the pose-driven deformation field can be controlled by input skeletal motions to generate new deformation fields to animate the canonical human model. Experiments show that our approach significantly outperforms recent human modeling methods.\n\n",
    "notes": "- Multi-View (Video) Reconstruction",
    "link": "https://arxiv.org/pdf/2203.08133",
    "codeLink": "https://zju3dv.github.io/animatable_sdf/",
    "bibtex": "@article{peng2024animatable,\n    title={Animatable Implicit Neural Representations for Creating Realistic Avatars from Videos},\n    author={Peng, Sida and Xu, Zhen and Dong, Junting and Wang, Qianqian and Zhang, Shangzhan and Shuai, Qing and Bao, Hujun and Zhou, Xiaowei},\n    journal={TPAMI},\n    year={2024},\n    publisher={IEEE}\n}"
  },
  {
    "title": "AniMer: Animal Pose and Shape Estimation Using Family Aware Transformer",
    "abstract": "Quantitative analysis of animal behavior and biomechanics requires accurate animal pose and shape estimation across species, and is important for animal welfare and biological research. However, the small network capacity of previous methods and limited multi-species dataset leave this problem underexplored. To this end, this paper presents AniMer to estimate animal pose and shape using family aware Transformer, enhancing the reconstruction accuracy of diverse quadrupedal families. A key insight of AniMer is its integration of a high-capacity Transformer-based backbone and an animal family supervised contrastive learning scheme, unifying the discriminative understanding of various quadrupedal shapes within a single framework. For effective training, we aggregate most available open-sourced quadrupedal datasets, either with 3D or 2D labels. To improve the diversity of 3D labeled data, we introduce CtrlAni3D, a novel large-scale synthetic dataset created through a new diffusion-based conditional image generation pipeline. CtrlAni3D consists of about 10k images with pixel-aligned SMAL labels. In total, we obtain 41.3k annotated images for training and validation. Consequently, the combination of a family aware Transformer network and an expansive dataset enables AniMer to outperform existing methods not only on 3D datasets like Animal3D and CtrlAni3D, but also on out-of-distribution Animal Kingdom dataset. Ablation studies further demonstrate the effectiveness of our network design and CtrlAni3D in enhancing the performance of AniMer for in-the-wild applications.",
    "notes": "- Single View Reconstruction\n- animal pose and shape estimation\n\n- AniMer network architecture. AniMer consists of  a ViT encoder that extracts image features; a transformer decoder that processes the image features from the encoder; a predictor head (MLPs) that generates animal family feature for supervised contrastive learning; and  a regression head (MLPs) that estimates the shape, pose, and camera parameters.\n-Generate Animal Image using ChatGPT and Controlnet: Condition images (depth, mask) as inputs (from SMAL sample)\n-> Cycle Consistency of SAM2(Controlnet_Output), and Condition Images ",
    "link": "https://arxiv.org/pdf/2412.00837",
    "codeLink": "https://luoxue-star.github.io/AniMer_project_page/",
    "bibtex": "@article{lyu2024animer,\n  title={AniMer: Animal Pose and Shape Estimation Using Family Aware Transformer},\n  author={Lyu, Jin and Zhu, Tianyi and Gu, Yi and Lin, Li and Cheng, Pujin and Liu, Yebin and Tang, Xiaoying and An, Liang},\n  journal={arXiv preprint arXiv:2412.00837},\n  year={2024}\n}"
  },
  {
    "title": "BANMo: Building Animatable 3D Neural Models from Many Casual Videos",
    "abstract": "Prior work for articulated 3D shape reconstruction often relies on specialized sensors (e.g., synchronized multi-camera systems), or pre-built 3D deformable models (e.g., SMAL or SMPL). Such methods are not able to scale to diverse sets of objects in the wild. We present BANMo, a method that requires neither a specialized sensor nor a pre-defined template shape. BANMo builds high-fidelity, articulated 3D models (including shape and animatable skinning weights) from many monocular casual videos in a differentiable rendering framework. While the use of many videos provides more coverage of camera views and object articulations, they introduce significant challenges in establishing correspondence across scenes with different backgrounds, illumination conditions, etc. Our key insight is to merge three schools of thought; (1) classic deformable shape models that make use of articulated bones and blend skinning, (2) volumetric neural radiance fields (NeRFs) that are amenable to gradient-based optimization, and (3) canonical embeddings that generate correspondences between pixels and an articulated model. We introduce neural blend skinning models that allow for differentiable and invertible articulated deformations. When combined with canonical embeddings, such models allow us to establish dense correspondences across videos that can be self-supervised with cycle consistency. On real and synthetic datasets, BANMo shows higher-fidelity 3D reconstructions than prior works for humans and animals, with the ability to render realistic images from novel viewpoints and poses.\n\n",
    "notes": "- reconstructs animated 3D shapes from video while also learning the skeleton structure. \n- These skeletons vary between animals\n- template free\n- mono video reconstruction",
    "link": "https://banmo-www.github.io/banmo-cvpr.pdf",
    "codeLink": "https://banmo-www.github.io/",
    "bibtex": "@inproceedings{yang2022banmo,\n  title={Banmo: Building animatable 3d neural models from many casual videos},\n  author={Yang, Gengshan and Vo, Minh and Neverova, Natalia and Ramanan, Deva and Vedaldi, Andrea and Joo, Hanbyul},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={2863--2873},\n  year={2022}\n}"
  },
  {
    "title": "GART: Gaussian Articulated Template Models",
    "abstract": "We introduce Gaussian Articulated Template Model\n(GART), an explicit, efficient, and expressive representation for non-rigid articulated subject capturing and rendering from monocular videos. GART utilizes a mixture of moving 3D Gaussians to explicitly approximate a deformable\nsubject’s geometry and appearance. It takes advantage of\na categorical template model prior (SMPL, SMAL, etc.)\nwith learnable forward skinning while further generalizing\nto more complex non-rigid deformations with novel latent\nbones. GART can be reconstructed via differentiable rendering from monocular videos in seconds or minutes and\nrendered in novel poses faster than 150fps.",
    "notes": "- uses geometry of BITE\n- template based\n- mono video reconstruction\n\n- shape is represented as a Gaussian Mixture Model (GMM)\n- each Gaussian’s transformation is linked to the bone transformations of an underlying skeleton\n-fur and clothing dynamics are simulated using a Multi-Layer Perceptron (MLP) that predicts \"neural bones\" to capture subtle deformations\n",
    "link": "https://arxiv.org/pdf/2311.16099",
    "codeLink": "https://github.com/JiahuiLei/GART",
    "bibtex": "@inproceedings{lei2024gart,\n  title={Gart: Gaussian articulated template models},\n  author={Lei, Jiahui and Wang, Yufu and Pavlakos, Georgios and Liu, Lingjie and Daniilidis, Kostas},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={19876--19887},\n  year={2024}\n}"
  },
  {
    "title": "Geometry Distributions",
    "abstract": "Neural representations of 3D data have been widely adopted across various applications, particularly in recent work leveraging coordinate-based networks to model scalar or vector fields. However, these approaches face inherent challenges, such as handling thin structures and non-watertight geometries, which limit their flexibility and accuracy. In contrast, we propose a novel geometric data representation that models geometry as distributions-a powerful representation that makes no assumptions about surface genus, connectivity, or boundary conditions. Our approach uses diffusion models with a novel network architecture to learn surface point distributions, capturing fine-grained geometric details. We evaluate our representation qualitatively and quantitatively across various object types, demonstrating its effectiveness in achieving high geometric fidelity. Additionally, we explore applications using our representation, such as textured mesh representation, neural surface compression, dynamic object modeling, and rendering, highlighting its potential to advance 3D geometric learning.",
    "notes": "- If already high resolution object is given. \n- New representation for storage reasons. ",
    "link": "https://arxiv.org/pdf/2411.16076",
    "codeLink": "https://1zb.github.io/GeomDist/",
    "bibtex": "@article{zhang2024geometry,\n  title={Geometry Distributions},\n  author={Zhang, Biao and Ren, Jing and Wonka, Peter},\n  journal={arXiv preprint arXiv:2411.16076},\n  year={2024}\n}"
  },
  {
    "title": "Hi-LASSIE: High-Fidelity Articulated Shape and Skeleton Discovery from Sparse Image Ensemble",
    "abstract": "Automatically estimating 3D skeleton, shape, camera\nviewpoints, and part articulation from sparse in-the-wild\nimage ensembles is a severely under-constrained and chal-\nlenging problem. Most prior methods rely on large-scale\nimage datasets, dense temporal correspondence, or human\nannotations like camera pose, 2D keypoints, and shape tem-\nplates. We propose Hi-LASSIE, which performs 3D articu-\nlated reconstruction from only 20-30 online images in the\nwild without any user-defined shape or skeleton templates.\nWe follow the recent work of LASSIE that tackles a similar\nproblem setting and make two significant advances. First,\ninstead of relying on a manually annotated 3D skeleton,\nwe automatically estimate a class-specific skeleton from the\nselected reference image. Second, we improve the shape\nreconstructions with novel instance-specific optimization\nstrategies that allow reconstructions to faithful fit on each instance while preserving the class-specific priors learned\nacross all images. Experiments on in-the-wild image en-\nsembles show that Hi-LASSIE obtains higher fidelity state-\nof-the-art 3D reconstructions despite requiring minimum\nuser input. ",
    "notes": "",
    "link": "https://arxiv.org/pdf/2212.11042",
    "codeLink": "https://github.com/google/hi-lassie"
  },
  {
    "title": "Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models",
    "abstract": "TL;DR: 2D Multi-view Diffusion Model and 3D diffusion-based Generative Model can be synchronized at diffusing and reverse sampling to provide complementary information to benefit each other.\n\nCreating realistic avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot provide multi-view shape priors with guaranteed 3D consistency. We propose Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion.\nOur key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other, and by coupling them in a tight manner, we can fully leverage the potential of both models. We introduce a novel image-conditioned generative 3D Gaussian Splats reconstruction model that leverages the priors from 2D multi-view diffusion models, and provides an explicit 3D representation, which further guides the 2D reverse sampling process to have better 3D consistency. Our design follows: (1) multi-view 2D priors enhance generative 3D reconstruction and (2) consistency refinement of diffusion sampling trajectory via the explicit 3D representation.",
    "notes": "- 2D View Generation Diffusion model, combined with Gaussian Splatting for 3D consistency. \n- Trained on human data. \n- Single View Reconstruction",
    "link": "https://arxiv.org/pdf/2406.08475",
    "codeLink": "https://yuxuan-xue.com/human-3diffusion/",
    "bibtex": "@article{xue2024human,\n  title={Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models},\n  author={Xue, Yuxuan and Xie, Xianghui and Marin, Riccardo and Pons-Moll, Gerard},\n  journal={arXiv preprint arXiv:2406.08475},\n  year={2024}\n}"
  },
  {
    "title": "LASSIE: Learning Articulated Shape from Sparse Image Ensemble via 3D Part Discovery",
    "abstract": "Creating high-quality articulated 3D models of animals is challenging either via manual creation or using 3D scanning tools. Therefore, techniques to reconstruct articulated 3D objects from 2D images are crucial and highly useful. In this work, we propose a practical problem setting to estimate 3D pose and shape of animals given only a few (10-30) in-the-wild images of a particular animal species (say, horse). Contrary to existing works that rely on pre-defined template shapes, we do not assume any form of 2D or 3D ground-truth annotations, nor do we leverage any multi-view or temporal information. Moreover, each input image ensemble can contain animal instances with varying poses, backgrounds, illuminations, and textures. Our key insight is that 3D parts have much simpler shape compared to the overall animal and that they are robust w.r.t. animal pose articulations. Following these insights, we propose LASSIE, a novel optimization framework which discovers 3D parts in a self-supervised manner with minimal user intervention. A key driving force behind LASSIE is the enforcing of 2D-3D part consistency using self-supervisory deep features. Experiments on Pascal-Part and self-collected in-the-wild animal datasets demonstrate considerably better 3D reconstructions as well as both 2D and 3D part discovery compared to prior arts.",
    "notes": "- optimization method that reconstructs 3D articulated animals given a set of images and a skeleton\n- It can be applied to generic animals, but cannot deal with occlusions and complex poses\n- It doesn’t look realistic but robotic.\n- Template Free\n- mono video reconstruction",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2022/file/6274d57365d7a6be06e58cad30d1b9da-Paper-Conference.pdf",
    "codeLink": "https://chhankyao.github.io/lassie/",
    "bibtex": "@inproceedings{yao2022lassie,\ntitle={LASSIE: Learning Articulated Shape from Sparse Image Ensemble via 3D Part Discovery},\nauthor={Yao, Chun-Han\n        and Hung, Wei-Chih\n        and Li, Yuanzhen\n        and Rubinstein, Michael\n        and Yang, Ming-Hsuan\n        and Jampani, Varun},\nbooktitle={NeurIPS},\nyear={2022}\n}"
  },
  {
    "title": "Millimetric Human Surface Capture in Minutes",
    "abstract": "Detailed human surface capture from multiple images is an essential component for many 3D production, analysis and transmission tasks. Yet producing millimetric precision 3D models in practical time, and actually verifying their 3D accuracy in a real-world capture context, remain key challenges due to the lack of specific methods and data for these goals. We propose two complementary contributions to this end. The first one is a highly scalable neural surface radiance field approach able to achieve millimetric precision by construction, while demonstrating high compute and memory efficiency. The second one is a novel dataset of clothed mannequin geometry captured with a high resolution hand-held 3D scanner paired with calibrated multi-view images, that allow to verify the millimetric accuracy claim.\n\nAlthough our approach can produce such a highly dense and precise geometry, we show how aggressive sparsification and optimizations of the neural surface pipeline lead to estimations requiring only minutes of computation time and a few GB of VRAM memory on GPU, while allowing for real-time millisecond neural rendering. On the basis of our framework and dataset, we provide a thorough experimental analysis of how such accuracies and efficiencies are achieved in the context of multi-camera human acquisition.",
    "notes": "multi-view (video) reconstruction",
    "link": "https://inria.hal.science/hal-04724016v2/file/SiggraphAsia2024-Millimetric-Humans-in-Minutes-preprint.pdf",
    "codeLink": "https://projects-morpheo.gitlabpages.inria.fr/millimetrichumans/",
    "bibtex": "@inproceedings{toussaint2024millimetric,\n  title={Millimetric Human Surface Capture in Minutes},\n  author={Toussaint, Briac and Boissieux, Laurence and Thomas, Diego and Boyer, Edmond and Franco, Jean-S{\\'e}bastien},\n  booktitle={SIGGRAPH Asia 2024 Conference Papers},\n  pages={1--12},\n  year={2024}\n}"
  },
  {
    "title": "TAVA: Template-free Animatable Volumetric Actors",
    "abstract": "Coordinate-based volumetric representations have the potential to generate photo-realistic virtual avatars from images. However,\nvirtual avatars also need to be controllable even to a novel pose that may\nnot have been observed. Traditional techniques, such as LBS, provide such\na function; yet it usually requires a hand-designed body template, 3D\nscan data, and limited appearance models. On the other hand, neural representation has been shown to be powerful in representing visual details,\nbut are under explored on deforming dynamic articulated actors. In this\npaper, we propose TAVA, a method to create Template-free Animatable\nV olumetric Actors, based on neural representations. We rely solely on\nmulti-view data and a tracked skeleton to create a volumetric model of\nan actor, which can be animated at the test time given novel pose. Since\nTAVA does not require a body template, it is applicable to humans as well\nas other creatures such as animals. Furthermore, TAVA is designed such\nthat it can recover accurate dense correspondences, making it amenable\nto content-creation and editing tasks. Through extensive experiments, we\ndemonstrate that the proposed method generalizes well to novel poses as\nwell as unseen views and showcase basic editing capabilities.",
    "notes": "- template free\n- mono video reconstruction\n- learns a 3D articulated model from multiple videos and a skeleton of an animal\n- it also learns skinning weights\n- results are reported only on noise-free synthetic images, with clean backgrounds, obtained from rigged graphics models\n- released data",
    "link": "https://arxiv.org/pdf/2206.08929",
    "codeLink": "https://github.com/facebookresearch/tava",
    "bibtex": "@inproceedings{li2022tava,\n  title={Tava: Template-free animatable volumetric actors},\n  author={Li, Ruilong and Tanke, Julian and Vo, Minh and Zollh{\\\"o}fer, Michael and Gall, J{\\\"u}rgen and Kanazawa, Angjoo and Lassner, Christoph},\n  booktitle={European Conference on Computer Vision},\n  pages={419--436},\n  year={2022},\n  organization={Springer}\n}"
  },
  {
    "title": "ViSER: Video-Specific Surface Embeddings for Articulated 3D Shape Reconstruction",
    "abstract": "We introduce ViSER, a method for recovering articulated 3D shapes and dense 3D trajectories from monocular videos. Previous work on high-quality reconstruction of dynamic 3D shapes typically relies on multiple camera views, strong category-specific priors, or 2D keypoint supervision. We show that none of these are required if one can reliably estimate long-range 2D point correspondences, making use of only 2D object masks and two-frame optical flow as inputs. ViSER infers correspondences by matching 2D pixels to a canonical, deformable 3D mesh via video-specific surface embeddings that capture the pixel appearance of each surface point. These embeddings behave as a continous set of keypoint descriptors defined over the mesh surface, which can be used to establish dense long-range correspondences across pixels. The surface embeddings are implemented via coordinate-based MLPs that are fit to each video via contrastive reconstruction losses. Experimental results show that ViSER compares favorably against prior work on challenging videos of humans with loose clothing and unusual poses as well as animals videos from DAVIS and YTVOS.",
    "notes": "- Inverse Rendering using embedded features on surface and image\n- Template-Free\n- mono video reconstruction",
    "link": "https://openreview.net/pdf?id=-JJy-Hw8TFB",
    "codeLink": "https://viser-shape.github.io/",
    "bibtex": "@article{yang2021viser,\n  title={Viser: Video-specific surface embeddings for articulated 3d shape reconstruction},\n  author={Yang, Gengshan and Sun, Deqing and Jampani, Varun and Vlasic, Daniel and Cole, Forrester and Liu, Ce and Ramanan, Deva},\n  journal={Advances in Neural Information Processing Systems},\n  volume={34},\n  pages={19326--19338},\n  year={2021}\n}"
  }
]